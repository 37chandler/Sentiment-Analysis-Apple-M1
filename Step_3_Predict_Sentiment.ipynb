{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A&A Project: Sentiment Analysis of Apple M1 in Twitter\n",
    "\n",
    "Author: Hongshen Lee\n",
    "\n",
    "Date:  2020/11/21\n",
    "\n",
    "## Step 3: Predict Sentiment and Summary Analysis\n",
    "\n",
    "This part is to load and use the model to predict the sentiment of those tweets collected in Step 1\n",
    "\n",
    "After that, I need to do some simple anslysis to finally be able to come to a decision making conclusion to answer:\n",
    "\n",
    "\"Should I buy the new Macbooks with M1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.classify import SklearnClassifier\n",
    "from string import punctuation\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.1: Data Phase\n",
    "\n",
    "Tweets datasets also need to be read, cleaned and featured before be transfered into the model to carry out the prediction.\n",
    "\n",
    "## Step 3.1.1:  Read Data\n",
    "\n",
    "### Fields\n",
    "For each record, it conains fields: \n",
    "\n",
    "- id_str: unique id for each tweet\n",
    "- username: twitter's screen name in twitter\n",
    "- location: twitter's location information in Twitter\n",
    "- following: the number of people twitter follows\n",
    "- followers: the number of people following the Twitter\n",
    "- total_tweers: the number of total tweets published by this Twitter\n",
    "- favorite_count: the number of the favorite clicks of this tweet\n",
    "- retweet_count: the number of retweets of this tweet.\n",
    "- text: the context of this tweet\n",
    "- source: Twitter uses what to publish this tweet.\n",
    "\n",
    "### Statistics:\n",
    "\n",
    "- This dataset contains 15308 records and 9249 unique twitters with 341 unique sources and 3798 unique locations.\n",
    "- The maximum count of followers is 18,295,495 and the average count of followers is 32885.33893. \n",
    "- The maximum count of favorite counts is 262 and the average count of favorite counts is 0.7. \n",
    "- The maximum retweet count is 4673 and the average retweet count is 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./data/apple_m1_data_new.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_from_csvFile(data_path):\n",
    "    data=pd.read_csv(data_path)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Carry out reading data and reformat data\n",
    "\n",
    "tweets_data=read_data_from_csvFile(data_path)\n",
    "# convert dataframe to the list of dict()\n",
    "tweets_data=list(tweets_data.T.to_dict().values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.1.2:  Clean Data\n",
    "\n",
    "- Lower case \n",
    "- Remove url, punctuation, stopwords\n",
    "- Remove words with tag like @ and #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(sentence):\n",
    "    remove_words=[\"macbook\",\"air\",\"m1\"]\n",
    "    sw = stopwords.words('english')\n",
    "    sentence = re.sub(r'(www.|https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '',\n",
    "                      sentence, flags=re.MULTILINE)                \n",
    "    sentence = [w.lower() for w in sentence.split() \n",
    "                if w.lower() not in sw\n",
    "                and not w.startswith('@')\n",
    "                and not w.startswith('#')]\n",
    "    sentence = [w for w in sentence if w not in punctuation]\n",
    "    sentence = [w for w in sentence if w not in remove_words]\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# carry out the cleaning\n",
    "for i in range(len(tweets_data)):\n",
    "    tweets_data[i][\"text\"]=clean_text(tweets_data[i][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['first', 'apple', 'silicon', '(m1)', 'macs', 'air,', 'pro', 'mac', 'mini']\n"
     ]
    }
   ],
   "source": [
    "# data sample\n",
    "print(tweets_data[125][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.2: Load Model and Predict\n",
    "\n",
    "### 3.2.1 Load Features and Model\n",
    "\n",
    "- Features are a list of the words\n",
    "- Model are generated by `nltk.NaiveBayesClassifier`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and apply the features to the data\n",
    "# Same as the one in Step one:\n",
    "def extract_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in w_features:\n",
    "        features['contains(%s)' % word] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41276\n"
     ]
    }
   ],
   "source": [
    "# Load features\n",
    "\n",
    "w_features=[]\n",
    "with open('features.txt',\"r\") as f:    \n",
    "    str = f.read()\n",
    "    features=str.split('\\n')\n",
    "print(len(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Models\n",
    "\n",
    "classifier = pickle.load(open('my_classifier.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Predict the tweets\n",
    "\n",
    "- Predict the sentiment of each tweet content\n",
    "- Replace the text field with the predicted sentiment to support following analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 nodejs apple silicon hardware &amp; iot blockforums\n",
      "0 problem directly, showing microsoft google soc option desktop/laptop too.\n",
      "0 southbound (#a52 (#a50 1 lane closed due animals carriageway. traffic officers currently scene.\n",
      "0 truly hate went 2016 new couple months ago camera quality absolute garbage lol\n",
      "0 saw account giveaway. told initial giveaway price 150 dollars gave that. showed picture address all. asked 150 dollars shipping. gave that. blocked me.\n",
      "0 3 new macs unboxed! air, pro 13in mac mini! via\n",
      "0 doesn‚Äôt charger port thing ü§®\n",
      "0 still using old looking new one, tell latest sickk affff rm 3.9k trust worth chip already leading comparing chip pro 16 inch price of12k\n",
      "0 /13\" messenger bag recs?\n",
      "0 review: right apple silicon mac\n",
      "0 yeah there's reason mac mini would issues! üò° home year old running fine it. even updated 2013 seems ok bit slow. i'm even sure genius bar open nyc apple stores.\n",
      "0 first off, hell pay $3k mac? second. go buy new air. fan man.\n",
      "0 updated big sur fan time i'm angry\n",
      "0 chip powerful 16-inch pro\n",
      "0 thanks energy efficiency chip, delivers powerful fanless performance, meaning matter users do, remains completely silent.\n",
      "0 apple looking skeptics tech reviewers saying claims chip ‚Äúselective‚Äù needed ‚Äúmore testing‚Äù üòÇ testing lol\n"
     ]
    }
   ],
   "source": [
    "# Carry out the prediction\n",
    "\n",
    "for i in range(len(tweets_data)):\n",
    "    sentiment = classifier.classify(extract_features(tweets_data[i][\"text\"]))\n",
    "    if(i%1000==0):\n",
    "        print(sentiment,\" \".join(tweets_data[i][\"text\"]))\n",
    "    tweets_data[i][\"text\"]=sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id_str': 1329980478628261888, 'username': 'BruceQBurke', 'location': 'Belleair Bluffs, FL', 'following': 11884, 'followers': 11170, 'totaltweets': 108269, 'favoritecount': 1, 'retweetcount': 0, 'text': 0, 'source': 'Twitter for iPhone'}\n",
      "{'id_str': 1328188766486360065, 'username': 'EvolvingViews', 'location': '22.367072,114.05898', 'following': 196, 'followers': 842, 'totaltweets': 4180, 'favoritecount': 1, 'retweetcount': 0, 'text': 0, 'source': 'Twitter Web App'}\n",
      "{'id_str': 1328387360011268096, 'username': 'teresawliao', 'location': 'Brooklyn', 'following': 319, 'followers': 223, 'totaltweets': 5245, 'favoritecount': 0, 'retweetcount': 0, 'text': 0, 'source': 'Twitter Web App'}\n"
     ]
    }
   ],
   "source": [
    "# Data sample\n",
    "print(tweets_data[1])\n",
    "print(tweets_data[1000])\n",
    "print(tweets_data[10000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.3: Summary And Reflection\n",
    "\n",
    "The title should be summary and anslysis, but something bad happened!\n",
    "\n",
    "### 1.  How many neg and pos sentiment ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Records:15307, POS:0.000000 NEG:1.000000\n"
     ]
    }
   ],
   "source": [
    "pos=0\n",
    "neg=0\n",
    "for i in range(len(tweets_data)):\n",
    "    if tweets_data[1][\"text\"]==1:\n",
    "        pos=pos+1\n",
    "    else:\n",
    "        neg=neg+1\n",
    "print(\"Total Records:{}, POS:{:f} NEG:{:f}\".format(len(tweets_data),pos/len(tweets_data),neg/len(tweets_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Sadly o(‚ï•Ôπè‚ï•)o, this result is too strange. Maybe this model can not be adapted to this dataset?\n",
    "\n",
    "**How many frequent words in the feature list?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "tweets_data=read_data_from_csvFile(data_path)\n",
    "# convert dataframe to the list of dict()\n",
    "tweets_data=list(tweets_data.T.to_dict().values())\n",
    "\n",
    "# carry out the cleaning\n",
    "for i in range(len(tweets_data)):\n",
    "    tweets_data[i][\"text\"]=clean_text(tweets_data[i][\"text\"])\n",
    "    \n",
    "words=[]\n",
    "for i in range(len(tweets_data)):\n",
    "    words.extend(tweets_data[i][\"text\"])\n",
    "    \n",
    "token_fd = FreqDist(words)\n",
    "frequent_words=[]\n",
    "\n",
    "for (word,fre) in token_fd.most_common(10000):\n",
    "    frequent_words.append(word)\n",
    "\n",
    "count=0\n",
    "for word in frequent_words:\n",
    "    if word in w_features:\n",
    "        count=count+1\n",
    "print(count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, this is the problem. Our words don't match the words in the Amazon reviews dataset.\n",
    "\n",
    "I once thought they are both reviews about the product.But I was wrong.\n",
    "\n",
    "Since I just features the 5000 records in that dataset, maybe if I increase the size, results would be improved.\n",
    "\n",
    "But not obviously, I think."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
