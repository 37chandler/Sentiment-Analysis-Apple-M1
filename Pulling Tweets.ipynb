{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pulling Data with Tweepy\n",
    "\n",
    "**By:** _Hongshen Lee_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import tweepy\n",
    "# I've put my API keys in a .py file called API_keys.py\n",
    "from API_keys import api_key, api_key_secret, access_token, access_token_secret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Authenticate the Tweepy API\n",
    "auth = tweepy.OAuthHandler(api_key,api_key_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "# Once the rate limit is hit, we will be notified that we must wait 15 mins (900 secs)\n",
    "api = tweepy.API(auth,wait_on_rate_limit=True,wait_on_rate_limit_notify=True, compression=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brief Introduction of Projects\n",
    "Apple has released its cross era product with **Apple Silicon M1**. I'm curious about what this new product looks like and what its reputation is. Using the API excuse provided by Tweepy, I grab the relevant records for further analysis.\n",
    "\n",
    "### Fields\n",
    "For each record, it conains fields: \n",
    "\n",
    "- id_str: unique id for each tweet\n",
    "- username: twitter's screen name in twitter\n",
    "- location: twitter's location infot in twitter\n",
    "- following: the number of people twitter follows\n",
    "- followers: the number of people following the twitter\n",
    "- total_tweers: the number of total tweets published by this twitter\n",
    "- favorite_count: the number of the favorite clicks of this tweet\n",
    "- retweet_count: the number of the retweets of this tweet.\n",
    "- text: the context of this tweet\n",
    "- source: twitter use what to publish this tweet.\n",
    "\n",
    "### Discussion\n",
    "\n",
    "For this dataset, serval interesting problems could be:\n",
    "\n",
    "- How people like or not the new product with new chips from Apple ?\n",
    "- Do active and influential people (with more followers and total tweets) like new products?\n",
    "- Do people's preferences have anything to do with their geographical location ？\n",
    "- Do people's preferences have anything to do with their mobilephones (coule be inferred by source field) ？\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search Relevant Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the limited number of API calls one can make using a basic and free developer account, (~900 calls every 15 minutes before your access is denied) Following methods created a function that extract 2,500 tweets per run once every 15 minutes\n",
    "\n",
    "Ref:https://medium.com/python-in-plain-english/scraping-tweets-with-tweepy-python-59413046e788"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file_name=\"apple_m1_data_new.csv\"\n",
    "def init_file():\n",
    "    db_tweets = pd.DataFrame(columns=['id_str','username', 'location', 'following',\n",
    "                                      'followers', 'total_tweets', 'favorite_count',\n",
    "                                      'retweet_count', 'text', 'source'])\n",
    "    db_tweets.to_csv(file_name, index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def write_down_records(tweet_list):\n",
    "    db_tweets = pd.DataFrame(columns=['id_str','username', 'location', 'following',\n",
    "                                      'followers', 'total_tweets', 'favorite_count',\n",
    "                                      'retweet_count', 'text', 'source'])\n",
    "    for tweet in tweet_list:\n",
    "        id_str = tweet.id_str\n",
    "        username = tweet.user.screen_name\n",
    "        location = tweet.user.location\n",
    "        following = tweet.user.friends_count\n",
    "        followers = tweet.user.followers_count\n",
    "        total_tweets = tweet.user.statuses_count\n",
    "        retweet_count = tweet.retweet_count\n",
    "        favorite_count = tweet.favorite_count\n",
    "        try:\n",
    "            text = tweet.full_text\n",
    "            source = tweet.source\n",
    "        except AttributeError:  # A Retweet\n",
    "            text = tweet.retweeted_status.full_text\n",
    "            source = tweet.retweeted_status.source\n",
    "        # Add the 11 variables to the empty list - ith_tweet:\n",
    "        ith_tweet = [id_str,username, location, following, followers, total_tweets,\n",
    "                 retweet_count, favorite_count, text, source]\n",
    "        db_tweets.loc[len(db_tweets)] = ith_tweet\n",
    "    db_tweets.to_csv(file_name, mode='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def scrape_tweets(search_words, date_since):\n",
    "    # Define a for-loop to generate tweets at regular intervals\n",
    "    # We cannot make large API call in one go. Hence, let's try T times\n",
    "\n",
    "    program_start = time.time()\n",
    "    # Collect tweets using the Cursor object\n",
    "    # .Cursor() returns an object that you can iterate or loop over to access the data collected.\n",
    "    count=1;\n",
    "    for tweets in tweepy.Cursor(api.search, q=search_words, lang=\"en\", rpp=100,count=100, \n",
    "                        since=date_since, tweet_mode='extended').pages():\n",
    "        # We will time how long it takes to scrape tweets for each run:\n",
    "        start_run = time.time()\n",
    "        # Store these tweets into a python list\n",
    "        tweet_list = [tweet for tweet in tweets]\n",
    "        noTweets = len(tweet_list)\n",
    "        # Wrtie down the records to file\n",
    "        write_down_records(tweet_list)\n",
    "        # Run ended:\n",
    "        duration_run = round((time.time() - start_run) / 60, 2)\n",
    "        print('Time take for {} to complete is {} mins,scraped {} tweets'.format(count,duration_run, noTweets))\n",
    "        count=count+1\n",
    "        # time.sleep(920)  # 15 minute sleep time\n",
    "\n",
    "    # End\n",
    "    program_end = time.time()\n",
    "    print('Scraping has completed!')\n",
    "    print('Total time taken to scrap is {} minutes.'.format(round(program_end - program_start) / 60, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key words to search\n",
    "Some keywords to search relevant tweets: like apple m1, mba, macbook air, apple chip.\n",
    "But these tweets are not necessarily related to our topic \"Apple's new chip\", so setting a since time can effectively improve the relevance of tweets to our topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "init_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "search_words = \"#M1 -filter:retweets\"\n",
    "date_since = \"2020-11-10\"\n",
    "# Call the function scraptweets\n",
    "scrape_tweets(search_words, date_since)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "search_words = \"MBA OR MACBOOK AIR -filter:retweets\"\n",
    "date_since = \"2020-11-10\"\n",
    "# Call the function scraptweets\n",
    "scrape_tweets(search_words, date_since)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time take for 1 to complete is 0.01 mins,scraped 100 tweets\n",
      "Time take for 2 to complete is 0.01 mins,scraped 100 tweets\n",
      "Time take for 3 to complete is 0.01 mins,scraped 100 tweets\n",
      "Time take for 4 to complete is 0.01 mins,scraped 100 tweets\n",
      "Time take for 5 to complete is 0.0 mins,scraped 59 tweets\n",
      "Scraping has completed!\n",
      "Total time taken to scrap is 0.55 minutes.\n"
     ]
    }
   ],
   "source": [
    "search_words = \"#Apple chip -filter:retweets\"\n",
    "date_since = \"2020-11-10\"\n",
    "# Call the function scraptweets\n",
    "scrape_tweets(search_words, date_since)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
